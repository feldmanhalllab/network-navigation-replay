bayesian_model_selection <- function(
    model_evidence, n_samples = 1e6, tidy_output = TRUE
) {
  # Bayesian model selection for group studies
  # Adapted from https://github.com/mattelisi/bmsR/
  # Standalone version: optional tidyverse dependency
  # 
  # Parameters:
  #   model_evidence: N (subjects) x K (models) matrix of LOG model evidence
  #   n_samples: # samples used in the simulation for calculating exceedance
  # 
  # Returns:
  #   alpha: posterior estimates of Dirichlet parameters
  #   expected_model_frequencies
  #   omnibus_risk: Bayesian omnibus risk
  #   xp: exceedance probabilities
  #   pxp: protected exceedance probabilities
  
  #### Helpers ####
  
  dirichlet_exceedance <- function(alpha, Nsamp=1e6) {
    
    repmat <- function(X, m, n) {
      mx = dim(X)[1]
      nx = dim(X)[2]
      matrix(t(matrix(X, mx, nx*n)), mx*m, nx*n, byrow=T)
    }
    
    Nk <- length(alpha)
    ind_max <- function(x) ifelse(x==max(x),1,0)
    xp <- rep(0, Nk)
    
    # Sample from univariate gamma densities then normalise
    # (see Dirichlet entry in Wikipedia or Ferguson (1973) Ann. Stat. 1,
    #   % 209-230)
    r = repmat(as.matrix(0), Nsamp, Nk)
    
    for (k in 1:Nk){
      r[,k] <- rgamma(Nsamp, alpha[k], scale = 1)
    }
    
    sr <- apply(r, 1, sum)
    
    for (k in 1:Nk){
      r[,k] <- r[,k]/sr
    }
    
    # Exceedance probabilities:
    # For any given model k1, compute the probability that it is more
    # likely than any other model k2~=k1
    j <- apply(r, 1, which.max)
    # xp <- xp + unname(tapply(j, factor(j),length)) # old
    label_j <- factor(j, levels=1:Nk)
    xp <- xp + unname(tapply(j, label_j,length))
    
    # if there's any model which was never the most frequent
    # then it's value would turn out to be NA here
    # so I cahnge it to zero
    xp <- ifelse(is.na(xp), 0, xp)
    xp <-xp/Nsamp
  }
  
  FE_null <- function(m){
    # Calculate free energy for H0 (equal model frequencies in the population)
    # Derives the free energy of the 'null' (H0: equal model frequencies)
    
    n <- dim(m)[1]  # number of subjects
    K <- dim(m)[2]  # number of models
    F0m <- 0
    for (i in 1:n) {
      tmp <- m[i,] - max(m[i,])
      g <- exp(tmp)/sum(exp(tmp))
      for (k in 1:K) {
        F0m = F0m + g[k] * (m[i,k]-log(K)-log(g[k]+.Machine$double.eps))
      }
    }
    return(unname(F0m))
  }
  
  FE <- function(m, a, g) {
    n <- dim(m)[1]  # number of subjects
    K <- dim(m)[2]  # number of models
    a0 <- rep(1, K)
    Elogr <- digamma(a)- digamma(sum(a))
    Sqf <- sum(lgamma(a)) - lgamma(sum(a)) - sum((a-1)*Elogr)
    Sqm <- 0
    for (i in 1:n) {
      Sqm <- Sqm - sum(g[i,] * log(g[i,]+.Machine$double.eps))
    }
    ELJ <- lgamma(sum(a0)) - sum(lgamma(a0)) + sum((a0-1)*Elogr)
    for (i in 1:n) {
      for (k in 1:K) {
        ELJ <- ELJ + g[i,k]*(Elogr[k]+m[i,k])
      }
    }
    F_ <- ELJ + Sqf + Sqm
    return(F_)
  }
  
  #### Main ####
  
  if (is.data.frame(model_evidence)) {
    m <- as.matrix(model_evidence)
  } else {
    m <- model_evidence
  }
  
  max_val <- log(.Machine$double.xmax)
  n_subjects <- dim(m)[1]
  n_models <- dim(m)[2]
  convergence_current <- 1
  convergence_threshold <- 10e-4
  
  alpha0 <- rep(1, n_models)
  
  log_u <- matrix(NA, n_subjects, n_models)
  u <- log_u
  g <- log_u
  alpha <- alpha0
  beta <- rep(NA, n_models)
  
  # Iterative variational Bayes estimation
  while(convergence_current > convergence_threshold){
    for(i in 1:n_subjects) {
      
      for(k in 1:n_models) {
        # Integrate out prior probabilities of models (in log space)
        log_u[i,k] <- m[i,k] + digamma(alpha[k])- digamma(sum(alpha))
        
        # Prevent numerical problems for badly-scaled posteriors
        log_u[i,k] <- sign(log_u[i,k]) * min(max_val, abs(log_u[i,k]))
      }
      
      # Exponentiate (to get back to non-log representation)
      u[i,] <- exp(log_u[i,])
      
      # Normalisation: sum across all models for i-th subject
      u_i <- sum(u[i,])
      g[i,] <- u[i,]/u_i
    }
    
    # Expected #subjects whose data we believe to have been generated by model k
    for(k in 1:n_models) {
      beta[k] <- sum(g[,k])
    }
    
    # Update alpha
    prev <- alpha
    for (k in 1:n_models) {
      alpha[k] <- alpha0[k] + beta[k]
    }
    
    # Convergence?
    convergence_current <- sqrt(sum((alpha - prev)^2))
  }
  
  # The posterior p(r|y)
  exp_r <- alpha/sum(alpha)
  
  # Exceedance probabilities
  xp <- dirichlet_exceedance(alpha, n_samples)
  
  # Bayes Omnibus Risk and Preotected xp
  F1 <- FE(m, alpha , g) # Evidence of alternative
  F0 <- FE_null(m) # Evidence of null (equal model freqs)
  
  # Implied by Eq 5 (see also p39) in Rigoux et al.
  # See also, last equation in Appendix 2
  bor <- 1 / (1+exp(F1-F0))
  
  # Compute protected exceedance probs - Eq 7 in Rigoux et al.
  pxp <- ((1-bor)*xp) + (bor/n_models)
  
  # Output
  out <- list(
    alpha = alpha,
    expected_model_frequencies = exp_r,
    omnibus_risk = bor,
    xp = xp,
    pxp = pxp
  )
  
  if (tidy_output) {
    out_rownames <- colnames(m)
    out <- dplyr::bind_cols(
      tibble::tibble(model = out_rownames),
      tibble::as_tibble(out)
    )
  }
  
  return(out)
}

